{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PczqZL0cY4GI",
        "outputId": "7be1fe08-8f23-48e4-d791-450d4c5d21f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=583849bfe89e618c96e4615900b9b513de3c13efe10c482be49a2caf254a362c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-RrUVFtXNdL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler, ChiSqSelector, OneHotEncoder\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Obesity_Level_Classification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = spark.read.csv(\"ObesityDataSet.csv\", header=True, inferSchema=True)\n",
        "# Handle missing values\n",
        "df = df.na.drop()\n",
        "\n",
        "# Select first 600 rows\n",
        "df = df.limit(600)\n",
        "\n",
        "# Collect the data for the NObeyesdad column to the driver\n",
        "nobeysdad_data = df.select(\"NObeyesdad\").toPandas()\n",
        "\n",
        "# Plot the distribution using matplotlib\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(nobeysdad_data[\"NObeyesdad\"], bins=20, color='skyblue', alpha=0.7)\n",
        "plt.title('Distribution of NObeyesdad Column')\n",
        "plt.xlabel('Obesity Level')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if necessary\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Select relevant columns\n",
        "selected_columns = ['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n",
        "                    'FAVC', 'FCVC', 'NCP', 'CAEC', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad']\n",
        "df = df.select(selected_columns)\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_columns = ['Gender', 'family_history_with_overweight', 'MTRANS']\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in categorical_columns]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Encode remaining categorical columns\n",
        "categorical_columns_remaining = ['FAVC', 'CAEC', 'SCC', 'CALC']\n",
        "indexers_remaining = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in categorical_columns_remaining]\n",
        "pipeline_remaining = Pipeline(stages=indexers_remaining)\n",
        "df = pipeline_remaining.fit(df).transform(df)\n",
        "\n",
        "\n",
        "# Assemble features\n",
        "feature_columns = ['Gender_index', 'Age', 'Height', 'Weight', 'family_history_with_overweight_index',\n",
        "                   'FAVC_index', 'FCVC', 'NCP', 'CAEC_index', 'CH2O', 'SCC_index', 'FAF', 'TUE', 'CALC_index', 'MTRANS_index']\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(df)\n",
        "df = scaler_model.transform(df)\n",
        "# Update feature_columns to include the encoded columns\n",
        "feature_columns += [column+\"_encoded\" for column in categorical_columns_remaining]\n",
        "\n",
        "# Encode target variable\n",
        "target_indexer = StringIndexer(inputCol=\"NObeyesdad\", outputCol=\"NObeyesdad_index\", handleInvalid=\"keep\")\n",
        "df = target_indexer.fit(df).transform(df)\n",
        "# Select relevant features using ChiSqSelector\n",
        "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"scaled_features\", outputCol=\"selected_features\",\n",
        "                         labelCol=\"NObeyesdad_index\")\n",
        "selector_model = selector.fit(df)\n",
        "df = selector_model.transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "(training_data, testing_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Initialize machine learning models\n",
        "rf = RandomForestClassifier(labelCol=\"NObeyesdad_index\", featuresCol=\"selected_features\")\n",
        "lr = LogisticRegression(labelCol=\"NObeyesdad_index\", featuresCol=\"selected_features\")\n",
        "\n",
        "# Train models\n",
        "rf_model = rf.fit(training_data)\n",
        "lr_model = lr.fit(training_data)\n",
        "\n",
        "\n",
        "# Evaluate models\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"NObeyesdad_index\", metricName=\"accuracy\")\n",
        "\n",
        "rf_predictions = rf_model.transform(testing_data)\n",
        "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
        "\n",
        "lr_predictions = lr_model.transform(testing_data)\n",
        "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
        "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
        "\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ]
    }
  ]
}